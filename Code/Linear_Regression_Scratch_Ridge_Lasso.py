# -*- coding: utf-8 -*-
"""ML_Project_LR_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eAWlg-LdXmkw2ME6RvjNVwBqIG5J0p-d
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing necessary Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from numpy.linalg import inv  
import mpl_toolkits
# %matplotlib inline

"""**Reading the Data**"""

data = pd.read_csv('Mumbai_House_Price.csv')

"""**Head of the data**"""

data.head()

"""**Dropping the Price (Target Variable)**"""

data_X = data.drop(['id', 'Price'],axis=1)

data_X.head()

"""**Taking Price Field in Y**"""

data_Y = data['Price']
data_Y.head()

"""**Chi Square Test For Feature Selection - Selecting 5 Features**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

Categorical_X = data_X.astype(int)

chi2_features = SelectKBest(chi2,k=5)
selected_X = chi2_features.fit_transform(Categorical_X, Y) # selected features

print('Ori: ', Categorical_X.shape[1])
print('Red: ', selected_X.shape[1])
print(selected_X)

"""**Important Features 1. Area 2. No of Bedrooms 3. Landscaped Gardens 4. Indoor Games 5. Gas Connection**"""

data_X.head()

"""**Dropping non-important features**"""

data_X = data.drop(['id','Price', 'No. of bathrooms', 'New/Resale','Gymnasium','Lift Available','Car Parking','Maintenance Staff','24x7 Security',"Children's Play Area",'Clubhouse','Intercom','Jogging Track','Swimming Pool'],axis=1)
data_X.head()

"""**Linear Regression for X (Selected 5 Featured Data) using built-in Library**"""

#Library Linear Regression for X (Selected 5 Featured Data)

from sklearn.linear_model import LinearRegression

linear_regression = LinearRegression()
data_Y = data['Price']

from sklearn.model_selection import train_test_split
X_train , X_test , Y_train , Y_test = train_test_split(data_X, data_Y, test_size = 0.20,random_state =2)

linear_regression.fit(X_train,Y_train)

linear_regression.score(X_train,Y_train)

linear_regression.score(X_test,Y_test)

X_train

def calculate_price(X_test, theta):
    
    price = np.dot(X_test, theta) 
    return price

N, D = X_train.shape
X_augmented = np.hstack([np.ones((N,1)), X_train]) 
print(X_augmented.shape)
MLE_augmented_parameter = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1

def calculate_augmented_theta(X_aug, Y_test):
    augmented_theta = np.zeros((D+1,1))

    X_T = X_augmented.transpose()
    X_T_X = np.dot(X_T, X_augmented)
    X_T_X_inverse = inv(X_T_X)
    X_T_X_inverse_X_T = np.dot(X_T_X_inverse, X_T)
    augmented_theta = np.dot(X_T_X_inverse_X_T, Y_train)
    
    return augmented_theta

augmented_theta = calculate_augmented_theta(X_augmented, Y_test)
print(augmented_theta)

X_test_augmented = np.hstack([np.ones((X_test.shape[0],1)), X_test]) 
Y_test_predicted = calculate_price(Xtest_aug, augmented_theta)

from sklearn.metrics import r2_score
r2_score(Y_test, Y_test_predicted)

X_train_augmented = np.hstack([np.ones((X_train.shape[0],1)), X_train])
Y_train_predicted = calculate_price(X_train_augmented, augmented_theta)
r2_score(Y_train, Y_train_predicted)

"""**Linear Regression - Mean Square Error**"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

linear_regression = LinearRegression()
linear_regression.fit(X_train, Y_train)

mse = cross_val_score(linear_regression, X_train, Y_train, scoring='neg_mean_squared_error', cv=5)
mse_linear_regression = np.mean(mse)
print(mse_linear_regression)

"""**Ridge Regression - Mean Square Error**"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

ridge_para = Ridge()
parameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}
ridge_regression = GridSearchCV(ridge_para, parameters, scoring='neg_mean_squared_error',cv=5)
ridge_regression.fit(X_train, Y_train)
ridge_regression.score(X_train, Y_train)
mse = cross_val_score(ridge_regression, X_train, Y_train, scoring='neg_mean_squared_error', cv=5)
mse_ridge_regression = np.mean(mse)
print(mse_ridge_regression)

print(ridge_regression.best_params_)
print(ridge_regression.best_score_)

"""**Lasso Regression - Mean Square Error**"""

from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

lasso_para = Lasso()
parameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}
lasso_regression=GridSearchCV(lasso_para, parameters, scoring='neg_mean_squared_error', cv=5)

lasso_regression.fit(X_train,Y_train)

mse = cross_val_score(lasso_regression, X_train, Y_train, scoring='neg_mean_squared_error', cv=5)
mse_lasso_regression = np.mean(mse)
print(mse_lasso_regression)

print(lasso_regression.best_params_)
print(lasso_regression.best_score_)

"""**Comparison of Linear, Ridge and Lasso Regression Models in terms of mean square errors**"""

types = ['Linear', 'Ridge', 'Lasso'] 
mean_square_errors = [mse_linear_regression, mse_ridge_regression, mse_lasso_regression] 
plt.bar(models, mean_square_errors) 
plt.xlabel('Regression') 
plt.ylabel('Mean Square Error') 
plt.show()